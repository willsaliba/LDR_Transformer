{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de3cdc60-6ca8-408c-8976-81abb8de5683",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### CONGREGATING DATA ###\n",
    "import os\n",
    "all_files = \"\"\n",
    "directory = \"mini_LDR\"\n",
    "\n",
    "#for each file in directory, append its contents to all_files string\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".ldr\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            all_files += file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38b2ee0e-f789-4f91-bda9-9a8da6fba1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRE-TOKENISATION USING GPT4 ###\n",
    "import regex as regx\n",
    "gpt4_pattern = regx.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "#https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "\n",
    "all_sub_units = regx.findall(gpt4_pattern, all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64dbfb77-7f5c-4dc0-9f18-944ad2fab024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48], [32, 33], [76, 69, 79, 67, 65, 68], [32, 77, 79, 68, 69, 76], [32, 65, 85, 84, 72, 79, 82], [32, 76, 69, 71, 79], [32, 115, 116, 97, 102, 102], [32, 40], [117, 110, 107, 110, 111, 119, 110], [41, 59]]\n"
     ]
    }
   ],
   "source": [
    "### CONVERT EACH CHAR TO UTF-8 RAW BYTES AND INITALISE TOKEN SET ###\n",
    "subUnits = []\n",
    "\n",
    "#convert each subunit into raw bytes, then their corresponding ints\n",
    "for subUnit in all_sub_units:\n",
    "    rawBytes = subUnit.encode(\"utf-8\")\n",
    "    int_subUnits = list(map(int, rawBytes))\n",
    "    subUnits.append(int_subUnits)\n",
    "\n",
    "print(subUnits[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2e62f8d-823c-48eb-99d4-4e6a8cdc9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCS: MOST FREQUENT PAIR & MERGE  ###\n",
    "\n",
    "#finding the frequency of adjacent pairs within each subUnit\n",
    "def get_counts(ids):\n",
    "    counts = {}\n",
    "    for subUnit in ids:\n",
    "        for pair in zip(subUnit, subUnit[1:]):     #pythonic way to iterate consec elements\n",
    "            counts[pair] = counts.get(pair, 0) + 1 #if pair DNE val defaulted to 0 \n",
    "    return counts\n",
    "\n",
    "#replacing all occurances of pair, within ids (token_subUnits), with new token idx\n",
    "def merge_tokens(ids, pair, idx):\n",
    "    newIDs = []\n",
    "    for subUnit in ids:\n",
    "        newUnit, i = [], 0\n",
    "        while i < len(subUnit):\n",
    "            if i < len(subUnit)-1 and subUnit[i] == pair[0] and subUnit[i+1] == pair[1]:\n",
    "                newUnit.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newUnit.append(subUnit[i])\n",
    "                i += 1\n",
    "        newIDs.append(newUnit)\n",
    "    return newIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eaf41406-7502-4900-8185-7eaa4e12c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 merges\n",
      "Processed 200 merges\n"
     ]
    }
   ],
   "source": [
    "### TRAINING THE TOKENIZER ###\n",
    "\n",
    "#setting hyper param\n",
    "vocab_size = 500\n",
    "num_merges = vocab_size - 256\n",
    "allSubUnits = subUnits\n",
    "\n",
    "#PERFORMING MERGES -> find most common pair, then merge it\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    frequencies = get_counts(allSubUnits)\n",
    "    topPair = max(frequencies, key=frequencies.get) #comparares on vals rather then keys\n",
    "    newID = 256 + i\n",
    "    allSubUnits = merge_tokens(allSubUnits, topPair, newID)\n",
    "    merges[topPair] = newID\n",
    "    #print statement to show progress\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1} merges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "994afebd-d5fb-4e0a-b018-e8953e983e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINALISING VOCABULARY -> using merges, add all new tokens to final vocab\n",
    "vocab = {}\n",
    "for idx in range(256): \n",
    "    vocab[idx] = bytes([idx]) #bytes needs list arg\n",
    "for (t0, t1), idx in merges.items(): #items() makes map traversable & in order of insertion\n",
    "    vocab[idx] = vocab[t0] + vocab[t1] #concatenating 'byte objects'\n",
    "\n",
    "###TRAINING COMPLETE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0795a-5575-40b2-a3da-3bf04accb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODE & DECODE FUNCTIONS LDR <-> Tokens ###\n",
    "\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\")) #raw bytes\n",
    "    \n",
    "    #loop will repeat until no more merges can be performed (if statement will exit\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        #want byte pair (key) inside stats that has lowest indx in merges dict (since low level stuff merged first)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        #if pair DNE nothing else can be merged\n",
    "        if pair not in merges:\n",
    "            break \n",
    "        #otherwise update tokens \n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "def decode(ids):\n",
    "    #concatenating all bytes together using all indexes(idx) in ids (list of byte objects)\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    # print(tokens)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1ab27bc6-f0a5-4349-a061-0683ee3b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count before: 301976\n",
      "Token count after: 210557\n",
      "\n",
      "\n",
      "First subUnits before and afterwards\n",
      "\n",
      "[[48], [32, 33], [76, 69, 79, 67, 65, 68], [32, 77, 79, 68, 69, 76], [32, 65, 85, 84, 72, 79, 82], [32, 76, 69, 71, 79], [32, 115, 116, 97, 102, 102], [32, 40], [117, 110, 107, 110, 111, 119, 110], [41, 59]]\n",
      "\n",
      "\n",
      "[[48], [303], [307], [370], [509], [613], [595], [572], [636], [584]]\n"
     ]
    }
   ],
   "source": [
    "### TESTING ###\n",
    "before, after = 0, 0\n",
    "for subUnit in subUnits: before += len(subUnit)\n",
    "for subUnit in allSubUnits: after += len(subUnit)\n",
    "print(f\"Token count before: {before}\\nToken count after: {after}\")\n",
    "\n",
    "print(\"\\n\\nFirst subUnits before and afterwards\\n\")\n",
    "print(subUnits[:10])\n",
    "print(\"\\n\")\n",
    "print(allSubUnits[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19647545-72d7-438f-bd4a-3b386f8fd8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
