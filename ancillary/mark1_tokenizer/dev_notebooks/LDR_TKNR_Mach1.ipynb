{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3cdc60-6ca8-408c-8976-81abb8de5683",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### CONGREGATING DATA ###\n",
    "import os\n",
    "all_files = \"\"\n",
    "directory = \"mini_LDR\"\n",
    "\n",
    "#for each file in directory, append its contents to all_files string\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".ldr\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            all_files += file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b2ee0e-f789-4f91-bda9-9a8da6fba1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PRE-TOKENISATION USING GPT4 ###\n",
    "import regex as regx\n",
    "gpt4_pattern = regx.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "#https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py\n",
    "\n",
    "all_sub_units = regx.findall(gpt4_pattern, all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64dbfb77-7f5c-4dc0-9f18-944ad2fab024",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERT EACH CHAR TO UTF-8 RAW BYTES AND INITALISE TOKEN SET ###\n",
    "subUnits = []\n",
    "\n",
    "#convert each subunit into raw bytes, then their corresponding ints\n",
    "for subUnit in all_sub_units:\n",
    "    rawBytes = subUnit.encode(\"utf-8\")\n",
    "    int_subUnits = list(map(int, rawBytes))\n",
    "    subUnits.append(int_subUnits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e62f8d-823c-48eb-99d4-4e6a8cdc9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCS: MOST FREQUENT PAIR & MERGE  ###\n",
    "\n",
    "#finding the frequency of adjacent pairs within each subUnit\n",
    "def get_counts(ids):\n",
    "    counts = {}\n",
    "    for subUnit in ids:\n",
    "        for pair in zip(subUnit, subUnit[1:]):     #pythonic way to iterate consec elements\n",
    "            counts[pair] = counts.get(pair, 0) + 1 #if pair DNE val defaulted to 0 \n",
    "    return counts\n",
    "\n",
    "#replacing all occurances of pair, within ids (token_subUnits), with new token idx\n",
    "def merge_tokens(ids, pair, idx):\n",
    "    newIDs = []\n",
    "    for subUnit in ids:\n",
    "        newUnit, i = [], 0\n",
    "        while i < len(subUnit):\n",
    "            if i < len(subUnit)-1 and subUnit[i] == pair[0] and subUnit[i+1] == pair[1]:\n",
    "                newUnit.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newUnit.append(subUnit[i])\n",
    "                i += 1\n",
    "        newIDs.append(newUnit)\n",
    "    return newIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf41406-7502-4900-8185-7eaa4e12c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 merges\n",
      "Processed 200 merges\n"
     ]
    }
   ],
   "source": [
    "### TRAINING THE TOKENIZER ###\n",
    "\n",
    "#setting hyper param\n",
    "vocab_size = 500\n",
    "num_merges = vocab_size - 256\n",
    "allSubUnits = subUnits\n",
    "\n",
    "#PERFORMING MERGES -> find most common pair, then merge it\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    frequencies = get_counts(allSubUnits)\n",
    "    topPair = max(frequencies, key=frequencies.get) #comparares on vals rather then keys\n",
    "    newID = 256 + i\n",
    "    allSubUnits = merge_tokens(allSubUnits, topPair, newID)\n",
    "    merges[topPair] = newID\n",
    "    #print statement to show progress\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1} merges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994afebd-d5fb-4e0a-b018-e8953e983e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FINALISING VOCABULARY ###\n",
    "vocab = {}\n",
    "#first 256 utf-8 tokens\n",
    "for idx in range(256): \n",
    "    vocab[idx] = bytes([idx]) #bytes needs list arg\n",
    "#using merges, add all new tokens to final vocab\n",
    "for (t0, t1), idx in merges.items(): #items() makes map traversable & in order of insertion\n",
    "    vocab[idx] = vocab[t0] + vocab[t1] #concatenating 'byte objects'\n",
    "\n",
    "### TRAINING COMPLETE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ceaba9c-1a7f-4736-aec2-e9d3d3787112",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INFERENCE CODE -> USED IN ENCODE & DECODE ###\n",
    "def get_counts_inf(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1 \n",
    "    return counts\n",
    "\n",
    "def merge_tokens_inf(ids, pair, idx):\n",
    "    newIDs, i = [], 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newIDs.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newIDs.append(ids[i])\n",
    "            i += 1\n",
    "    return newIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e0795a-5575-40b2-a3da-3bf04accb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success: True\n"
     ]
    }
   ],
   "source": [
    "### ENCODE & DECODE FUNCTIONS LDR <-> Tokens ###\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\")) #raw bytes\n",
    "    #loop till no more merges possible\n",
    "    while len(tokens) >= 2:\n",
    "        frequencies = get_counts_inf(tokens)\n",
    "        #want byte pair (key) inside stats that has lowest indx in merges dict (lowest merged first)\n",
    "        pair = min(frequencies, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break \n",
    "        idx = merges[pair]\n",
    "        tokens = merge_tokens_inf(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "def decode(tokens):\n",
    "    #processing each token by decoding it with\n",
    "    byteTokens = b\"\".join(vocab[token] for token in tokens)\n",
    "    text = byteTokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab27bc6-f0a5-4349-a061-0683ee3b9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count before: 25246\n",
      "Count after: 17928\n",
      "\n",
      "BEFORE: [[48], [32, 33], [76, 69, 79, 67, 65, 68], [32, 77, 79, 68, 69, 76], [32, 65, 85, 84, 72, 79, 82], [32, 76, 69, 71, 79], [32, 115, 116, 97, 102, 102], [32, 40], [117, 110, 107, 110, 111, 119, 110], [41, 59]]\n",
      "\n",
      "AFTER: [[48], [290], [294], [325], [426], [455], [458], [382], [462], [384]]\n",
      "\n",
      "Encode & Decode Success: True\n"
     ]
    }
   ],
   "source": [
    "### TESTING ###\n",
    "before, after = 0, 0\n",
    "for subUnit in subUnits: before += len(subUnit)\n",
    "for subUnit in allSubUnits: after += len(subUnit)\n",
    "print(f\"Count before: {before}\\nCount after: {after}\")\n",
    "\n",
    "print(f\"\\nBEFORE: {subUnits[:10]}\")\n",
    "print(f\"\\nAFTER: {allSubUnits[:10]}\")\n",
    "\n",
    "text = \"0 !LEOCAD MODEL AUTHOR LEGO staff (unknown);\"\n",
    "theTokens = encode(text);\n",
    "result = decode(theTokens)\n",
    "print(f\"\\nEncode & Decode Success: {result == text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19647545-72d7-438f-bd4a-3b386f8fd8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
