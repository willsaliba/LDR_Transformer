{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54528b41",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tokens:  384\n"
     ]
    }
   ],
   "source": [
    "#converting chars to tokens using utr-8 ids\n",
    "text = \"hello world its me will and I'm a cool guy with lots to say, just kidding im actually very humble. The reason i said that is because i am trying to create a really long string with lots of words so i can train this tokeniser. pretty genius way of creating text dont you think? i'm literally just wrtiting out my inner dialogue. Ok im going to stop now because im feeling kind of crazy\"\n",
    "tokens = text.encode(\"utf-8\") #raw bytes\n",
    "# print(tokens)\n",
    "tokens = list(map(int, tokens)) #bytes to ints\n",
    "print(\"# tokens: \", len(tokens))\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853d2a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, (105, 110)), (10, (32, 116)), (10, (32, 105)), (8, (121, 32)), (8, (110, 103)), (8, (103, 32)), (7, (115, 32)), (7, (32, 119)), (6, (116, 32)), (6, (109, 32)), (5, (117, 115)), (5, (116, 111)), (5, (116, 104)), (5, (114, 101)), (5, (111, 32)), (5, (108, 111)), (5, (108, 108)), (5, (105, 116)), (5, (101, 32)), (5, (32, 115)), (5, (32, 99)), (5, (32, 97)), (4, (115, 116)), (4, (101, 114)), (4, (101, 97)), (4, (100, 32)), (4, (97, 108)), (4, (32, 111)), (4, (32, 108)), (3, (119, 105)), (3, (116, 115)), (3, (116, 114)), (3, (116, 105)), (3, (116, 101)), (3, (115, 101)), (3, (114, 97)), (3, (111, 110)), (3, (111, 102)), (3, (110, 32)), (3, (108, 121)), (3, (105, 115)), (3, (105, 109)), (3, (105, 32)), (3, (102, 32)), (3, (99, 114)), (3, (99, 97)), (3, (97, 116)), (3, (46, 32)), (3, (32, 103)), (2, (119, 111)), (2, (115, 111)), (2, (115, 97)), (2, (114, 121)), (2, (111, 117)), (2, (111, 116)), (2, (111, 114)), (2, (110, 105)), (2, (110, 100)), (2, (108, 105)), (2, (108, 32)), (2, (107, 105)), (2, (106, 117)), (2, (105, 100)), (2, (104, 105)), (2, (104, 101)), (2, (104, 32)), (2, (103, 117)), (2, (101, 110)), (2, (101, 108)), (2, (101, 99)), (2, (101, 46)), (2, (100, 105)), (2, (98, 101)), (2, (97, 121)), (2, (97, 117)), (2, (97, 110)), (2, (97, 105)), (2, (97, 32)), (2, (39, 109)), (2, (32, 114)), (2, (32, 109)), (2, (32, 107)), (2, (32, 106)), (2, (32, 100)), (2, (32, 98)), (1, (122, 121)), (1, (121, 111)), (1, (121, 105)), (1, (121, 44)), (1, (120, 116)), (1, (119, 114)), (1, (119, 97)), (1, (119, 32)), (1, (118, 101)), (1, (117, 121)), (1, (117, 116)), (1, (117, 109)), (1, (117, 101)), (1, (117, 97)), (1, (117, 32)), (1, (116, 121)), (1, (116, 117)), (1, (116, 116)), (1, (114, 116)), (1, (114, 108)), (1, (114, 105)), (1, (114, 100)), (1, (114, 46)), (1, (114, 32)), (1, (112, 114)), (1, (112, 32)), (1, (111, 119)), (1, (111, 112)), (1, (111, 111)), (1, (111, 108)), (1, (111, 107)), (1, (111, 105)), (1, (111, 103)), (1, (110, 116)), (1, (110, 111)), (1, (110, 110)), (1, (110, 107)), (1, (110, 101)), (1, (109, 121)), (1, (109, 101)), (1, (109, 98)), (1, (108, 101)), (1, (108, 100)), (1, (107, 101)), (1, (107, 63)), (1, (107, 32)), (1, (105, 117)), (1, (105, 108)), (1, (105, 97)), (1, (105, 39)), (1, (104, 117)), (1, (104, 97)), (1, (103, 111)), (1, (103, 101)), (1, (102, 101)), (1, (101, 120)), (1, (101, 116)), (1, (101, 101)), (1, (100, 115)), (1, (100, 111)), (1, (100, 100)), (1, (99, 116)), (1, (99, 111)), (1, (98, 108)), (1, (97, 122)), (1, (97, 115)), (1, (97, 109)), (1, (97, 99)), (1, (84, 104)), (1, (79, 107)), (1, (73, 39)), (1, (63, 32)), (1, (44, 32)), (1, (32, 121)), (1, (32, 118)), (1, (32, 112)), (1, (32, 110)), (1, (32, 104)), (1, (32, 102)), (1, (32, 84)), (1, (32, 79)), (1, (32, 73))]\n"
     ]
    }
   ],
   "source": [
    "#finding most common pairs\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): #pythonic way to iterate consec elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1 #if pair DNE val defaulted to 0 \n",
    "    return counts\n",
    "\n",
    "#counting paired tokens & then printing them in most frequent order\n",
    "stats = get_stats(tokens)\n",
    "print( sorted( ((v,k) for k,v in stats.items()) , reverse=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc33149c-e16c-404e-8c1a-1042e7b171fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "#in list of ids (ints), replace all consecutive occurances of pair(two chars) with new token idx(int)\n",
    "def merge(ids, pair, idx):\n",
    "    newIDs = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        #if not at end & pair matches, replace it\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newIDs.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newIDs.append(ids[i])\n",
    "            i += 1\n",
    "    return newIDs\n",
    "\n",
    "print( merge([5, 6, 6, 7, 9, 1], (6, 7), 99) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee555e82-7072-4ede-b1b1-d32082106a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging recurring bits\n",
    "vocab_size = 256 + 100 #256 from utf8 then option for 20 merges\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) #creating copy\n",
    "\n",
    "merges = {}\n",
    "#for num_merges, find most common pair, create new idx, update the output (ids) and store the merge in dict (merges)\n",
    "for i in range(num_merges):\n",
    "    #find most common pair\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get) #getting the current top pair\n",
    "    #create new idx, update the output (ids)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, pair, idx)\n",
    "    #store curr merge in dict (merges)\n",
    "    merges[pair] = idx\n",
    "\n",
    "# print(f\"before: {len(tokens)} after: {len(ids)} ratio: {len(tokens)/len(ids):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf4ec909-9d41-4d23-bad4-cd25a2cce3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create hashmap of key(idx)-value(byte object) pair -> when printed outputs a char\n",
    "vocab = {}\n",
    "#first add all regular 256 utf-8 chars\n",
    "for idx in range(256):\n",
    "    vocab[idx] = bytes([idx]) \n",
    "\n",
    "#next add all merged tokens\n",
    "for (p0, p1), idx in merges.items(): #runs in order of insertion\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] #concatenates 'byte objects'\n",
    "\n",
    "# \"training complete\" -> merges performed and vocab created (partially using merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6fd66b5-48a7-4149-9101-d531f1603410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given tokens (list of ints), return python string\n",
    "def decode(ids):\n",
    "    #concatenating all bytes together using all indexes(idx) in ids (list of byte objects)\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    # print(tokens)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae49fe3a-74e4-4a91-8db5-97a9fc1e741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given string, return list of ints (tokens)\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\")) #converting text to raw bytes\n",
    "    #loop will repeat until no more merges can be performed (if statement will exit\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        #want byte pair (key) inside stats that has lowest indx in merges dict (since low level stuff merged first)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        #if pair DNE nothing else can be merged\n",
    "        if pair not in merges:\n",
    "            break \n",
    "        #otherwise update tokens \n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14bf4ee7-6a4f-4a18-ab2b-a34bdfa45535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))\n",
    "print(text == decode(encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "745fc1c6-45cf-4cf0-b00e-6b5775105956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINISHED BASIC IMPLEMENTATION OF BYTE PAIR TOKENISER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049b9ab-e04f-4728-a708-b5c81d1cc300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
